train.batch_size=128
eval.batch_size=128
train.num_epochs=0
# Download the following model from RULSTM
# https://iplab.dmi.unict.it/sharing/rulstm/ek100_models/RULSTM-anticipation_0.25_6_8_rgb_mt5r_best.pth.tar
train.init_from_model=[[temporal_aggregator,/path/to/RULSTM-anticipation_0.25_6_8_rgb_mt5r_best.pth.tar],[classifiers.action,classifier.1.,/path/to/RULSTM-anticipation_0.25_6_8_rgb_mt5r_best.pth.tar]]

model/backbone=identity
model.backbone_dim=1024
model/temporal_aggregator=rulstm
model.temporal_aggregator.num_pad_feats=3
model.dropout=0.8

opt.lr_wd=[[backbone,0.0,0.0],[temporal_aggregator,0.01,0.0],[classifiers,0.01,0.0]]
opt.bias_bn_wd_scale=0.0
opt.optimizer.nesterov=true

data_train.num_frames=11
data_train.frame_rate=30
data_eval.num_frames=11
data_eval.frame_rate=30

opt/scheduler=cosine

dataset@dataset_train=epic_kitchens100/anticipation_train
dataset@dataset_eval=epic_kitchens100/anticipation_test
dataset_train.sample_strategy=last_clip
dataset_eval.sample_strategy=last_clip
dataset_train.conv_to_anticipate_fn.tau_o=2.5
dataset_eval.conv_to_anticipate_fn.tau_o=2.5
dataset.epic_kitchens100.common.label_type=action
+dataset_train.reader_fn={_target_: datasets.epic_kitchens.EpicRULSTMFeatsReader, lmdb_path: ${dataset.epic_kitchens100.common.rulstm_feats_dir}/rgb/, read_type: exact_rulstm}
+dataset_eval.reader_fn=${dataset_train.reader_fn}

# RULSTM data
+dataset_eval.conv_to_anticipate_fn.drop_style=correct

hydra.launcher.nodes=1
hydra.launcher.gpus_per_node=1

test_only=True
